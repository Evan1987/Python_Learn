{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent1 = \"The cat is walking in the bedroom.\"\n",
    "sent2 = \"A dog was running across the kitchen.\"\n",
    "sents = [sent1, sent2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 1 0 0 2 1 0]\n",
      " [1 0 0 1 0 0 1 1 1 0 1]]\n",
      "['across', 'bedroom', 'cat', 'dog', 'in', 'is', 'kitchen', 'running', 'the', 'walking', 'was']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "print(count_vec.fit_transform(sents).toarray())\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'walk', 'in', 'the', 'bedroom', '.']\n",
      "['A', 'dog', 'wa', 'run', 'across', 'the', 'kitchen', '.']\n",
      "[('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('bedroom', 'NN'), ('.', '.')]\n",
      "[('A', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('running', 'VBG'), ('across', 'IN'), ('the', 'DT'), ('kitchen', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# 对句子进行分割正规化\n",
    "tokens1 = nltk.word_tokenize(sent1)\n",
    "tokens2 = nltk.word_tokenize(sent2)\n",
    "# 整理词汇表\n",
    "vocab1 = sorted(set(tokens1))\n",
    "vocab2 = sorted(set(tokens2))\n",
    "# 寻找各词汇的原始词根\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stem1 = [stemmer.stem(t) for t in tokens1]\n",
    "stem2 = [stemmer.stem(t) for t in tokens2]\n",
    "print(stem1)\n",
    "print(stem2)\n",
    "# 初始化词性标注器，对每个词汇进行标注\n",
    "pos_tag1 = nltk.tag.pos_tag(tokens1)\n",
    "pos_tag2 = nltk.tag.pos_tag(tokens2)\n",
    "print(pos_tag1)\n",
    "print(pos_tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = news.data, news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将每条新闻中的句子逐一剥离， 并返回一个句子列表\n",
    "def news_to_sent(news):\n",
    "    news_text = BeautifulSoup(news).get_text()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sent = tokenizer.tokenize(news_text)\n",
    "    result = [re.sub('[^a-zA-Z]',' ',sent.lower().strip()).split() for sent in raw_sent]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in x:\n",
    "    sentences += news_to_sent(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量模型训练\n",
    "model = word2vec.Word2Vec(sentences=sentences, \n",
    "                          workers=2, \n",
    "                          size=300, \n",
    "                          min_count=20, \n",
    "                          window=5, \n",
    "                          sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 表示当前训练好的词向量为最终版，可加快训练速度\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afternoon', 0.795380711555481),\n",
       " ('weekend', 0.7631502151489258),\n",
       " ('evening', 0.7528274059295654),\n",
       " ('saturday', 0.7292416095733643),\n",
       " ('night', 0.705287754535675),\n",
       " ('friday', 0.6900104880332947),\n",
       " ('newspaper', 0.6784282326698303),\n",
       " ('summer', 0.6652509570121765),\n",
       " ('sunday', 0.6510100364685059),\n",
       " ('week', 0.6430712938308716),\n",
       " ('monday', 0.6334890723228455),\n",
       " ('month', 0.6218483448028564),\n",
       " ('thursday', 0.6154978275299072),\n",
       " ('tuesday', 0.614425778388977),\n",
       " ('yesterday', 0.6131061911582947),\n",
       " ('season', 0.6128036975860596),\n",
       " ('november', 0.6029937863349915),\n",
       " ('july', 0.5953641533851624),\n",
       " ('february', 0.5948885679244995),\n",
       " ('century', 0.5914508104324341)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用训练好的模型，寻找文本中与‘morning’最相关的 20个词汇\n",
    "model.most_similar('morning', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mail', 0.7357809543609619),\n",
       " ('contact', 0.7029544115066528),\n",
       " ('mailed', 0.6643069982528687),\n",
       " ('replies', 0.6533781290054321),\n",
       " ('address', 0.6261845827102661),\n",
       " ('send', 0.6236612796783447),\n",
       " ('request', 0.623630166053772),\n",
       " ('archie', 0.62105393409729),\n",
       " ('listserv', 0.6111855506896973),\n",
       " ('internet', 0.6090856790542603),\n",
       " ('sas', 0.6070916652679443),\n",
       " ('compuserve', 0.6016649007797241),\n",
       " ('fax', 0.6003612279891968),\n",
       " ('snail', 0.579025387763977),\n",
       " ('rend', 0.5787627696990967),\n",
       " ('ftp', 0.573432207107544),\n",
       " ('subscription', 0.5726449489593506),\n",
       " ('finger', 0.5684045553207397),\n",
       " ('amin', 0.567340612411499),\n",
       " ('kusmierczak', 0.5647874474525452)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('email', topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
